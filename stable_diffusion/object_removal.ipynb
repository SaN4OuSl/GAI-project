{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from numpy.ma.core import negative\n",
    "from transformers import AutoImageProcessor, YolosForObjectDetection, CLIPTokenizer\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from PIL import Image\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load YOLO model for object detection\n",
    "processor = AutoImageProcessor.from_pretrained(\"hustvl/yolos-small\")\n",
    "model = YolosForObjectDetection.from_pretrained(\"hustvl/yolos-small\").to(device)\n",
    "\n",
    "# Load Stable Diffusion Inpainting pipeline\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-inpainting\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "pipe.to(device)\n",
    "\n",
    "# Load tokenizer manually\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", use_fast=True)\n",
    "pipe.tokenizer = tokenizer\n",
    "\n",
    "\n",
    "# Ensure height and width are divisible by 8\n",
    "def adjust_size(width, height):\n",
    "    return (width // 8) * 8, (height // 8) * 8\n",
    "\n",
    "\n",
    "# Load image function\n",
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(\"Image not found! Check the file path.\")\n",
    "\n",
    "    print(\"Image loaded successfully.\")\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image_rgb, image\n",
    "\n",
    "\n",
    "selected_boxes = []\n",
    "object_boxes = []\n",
    "image_for_display = None\n",
    "\n",
    "\n",
    "# Mouse click event handler\n",
    "def select_object(event, x, y, flags, param):\n",
    "    global selected_boxes, object_boxes, image_for_display\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        for i, (x1, y1, x2, y2) in enumerate(object_boxes):\n",
    "            if x1 <= x <= x2 and y1 <= y <= y2:\n",
    "                selected_boxes.append(i)\n",
    "                print(f\"Selected object {i + 1} for removal.\")\n",
    "\n",
    "        temp_image = image_for_display.copy()\n",
    "        for i in selected_boxes:\n",
    "            x1, y1, x2, y2 = object_boxes[i]\n",
    "            cv2.rectangle(temp_image, (x1, y1), (x2, y2), (0, 0, 255), 3)\n",
    "\n",
    "        cv2.imshow(\"Select Objects to Remove\", temp_image)\n",
    "        cv2.startWindowThread()\n",
    "\n",
    "\n",
    "# Object detection and selection with YOLO\n",
    "def yolo_object_detection(image_rgb, image, padding=4):\n",
    "    global object_boxes, image_for_display\n",
    "\n",
    "    inputs = processor(images=image_rgb, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    target_sizes = torch.tensor([image_rgb.shape[:2]], device=device)\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.5)[0]\n",
    "\n",
    "    object_boxes = [box.int().tolist() for box in results[\"boxes\"]]\n",
    "\n",
    "    # Draw bounding boxes on a copy of the image\n",
    "    image_for_display = image.copy()\n",
    "    for x1, y1, x2, y2 in object_boxes:\n",
    "        cv2.rectangle(image_for_display, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "    # Open window for selection\n",
    "    cv2.imshow(\"Select Objects to Remove\", image_for_display)\n",
    "    cv2.setMouseCallback(\"Select Objects to Remove\", select_object)\n",
    "    print(\"\\nClick on the objects you want to remove. Press 'Enter' when done.\")\n",
    "\n",
    "    # Wait until Enter key is pressed\n",
    "    while True:\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 13:\n",
    "            detected_image_path = os.path.join(\"detected_objects/\", f\"{image_name}\")\n",
    "            cv2.imwrite(detected_image_path, cv2.cvtColor(image_for_display, cv2.COLOR_RGBA2RGB))\n",
    "            cv2.destroyAllWindows()\n",
    "            cv2.waitKey(1)\n",
    "            break\n",
    "\n",
    "    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    for i in selected_boxes:\n",
    "        x1, y1, x2, y2 = object_boxes[i]\n",
    "\n",
    "        x1 = max(0, x1 - padding)\n",
    "        y1 = max(0, y1 - padding)\n",
    "        x2 = min(image.shape[1], x2 + padding)\n",
    "        y2 = min(image.shape[0], y2 + padding)\n",
    "\n",
    "        mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "# Inpainting using Stable Diffusion\n",
    "def stable_diffusion_inpainting(image_rgb, mask):\n",
    "    # Ensure dimensions are divisible by 8\n",
    "    width, height = adjust_size(image_rgb.shape[1], image_rgb.shape[0])\n",
    "\n",
    "    # Resize images while maintaining aspect ratio\n",
    "    image_pil = Image.fromarray(image_rgb).resize((width, height))\n",
    "    mask_pil = Image.fromarray(mask).resize((width, height))\n",
    "\n",
    "    # Run Stable Diffusion inpainting\n",
    "    inpainted_image = pipe(\n",
    "        prompt=(\n",
    "            \"Fill the missing background of the image naturally. \"\n",
    "            \"Maintain proper background lighting.\"\n",
    "        ),\n",
    "        negative_prompt=(\n",
    "            \"Do not introduce any new objects, patterns, or artificial elements.\"\n",
    "        ),\n",
    "        image=image_pil,\n",
    "        mask_image=mask_pil,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        num_inference_steps=100\n",
    "    ).images[0]\n",
    "\n",
    "    # Convert result to NumPy array\n",
    "    inpainted_array = np.array(inpainted_image)\n",
    "\n",
    "    # Resize inpainted image & mask back to original size\n",
    "    inpainted_array_resized = cv2.resize(inpainted_array, (image_rgb.shape[1], image_rgb.shape[0]),\n",
    "                                         interpolation=cv2.INTER_LANCZOS4)\n",
    "    mask_resized = cv2.resize(mask, (image_rgb.shape[1], image_rgb.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Blend the inpainted result only in masked areas\n",
    "    final_result = image_rgb.copy()\n",
    "    final_result[mask_resized > 0] = inpainted_array_resized[mask_resized > 0]\n",
    "\n",
    "    return final_result\n",
    "\n",
    "\n",
    "def blend_edges(original, inpainted, mask):\n",
    "    blurred_mask = cv2.GaussianBlur(mask.astype(np.float32), (15, 15), 0)\n",
    "    blended = original * (1 - blurred_mask[:, :, None] / 255) + inpainted * (blurred_mask[:, :, None] / 255)\n",
    "    return blended.astype(np.uint8)\n",
    "\n",
    "\n",
    "def save_results(mask, output_image, output_image_blended):\n",
    "    mask_pil = Image.fromarray(mask)\n",
    "    output_pil = Image.fromarray(output_image)\n",
    "    output_blended_pil = Image.fromarray(output_image_blended)\n",
    "\n",
    "    mask_pil.save(\"masks/\" + image_name)\n",
    "    output_pil.save(\"outputs/\" + image_name)\n",
    "    output_blended_pil.save(\"outputs_blended/\" + image_name)\n",
    "    print(\"Mask and output image saved successfully.\")\n",
    "\n",
    "\n",
    "image_name = \"bikes.jpg\"\n",
    "\n",
    "try:\n",
    "    image_rgb, image = load_image(\"../input_images/\" + image_name)\n",
    "    mask = yolo_object_detection(image_rgb.copy(), image.copy())\n",
    "    stable_diffused_result = stable_diffusion_inpainting(image_rgb, mask)\n",
    "    blended_result = blend_edges(image_rgb, stable_diffused_result, mask)\n",
    "    save_results(mask, stable_diffused_result, blended_result)\n",
    "\n",
    "    current_pid = os.getpid()\n",
    "    for proc in psutil.process_iter(attrs=['pid', 'name']):\n",
    "        try:\n",
    "            if \"python\" in proc.info['name'].lower() and proc.info['pid'] != current_pid:\n",
    "                os.kill(proc.info['pid'], 9)\n",
    "        except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "            continue\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 179\u001B[0m\n\u001B[1;32m    177\u001B[0m image_rgb, image \u001B[38;5;241m=\u001B[39m load_image(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../input_images/\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m image_name)\n\u001B[1;32m    178\u001B[0m mask \u001B[38;5;241m=\u001B[39m yolo_object_detection(image_rgb\u001B[38;5;241m.\u001B[39mcopy(), image\u001B[38;5;241m.\u001B[39mcopy())\n\u001B[0;32m--> 179\u001B[0m stable_diffused_result \u001B[38;5;241m=\u001B[39m \u001B[43mstable_diffusion_inpainting\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_rgb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m blended_result \u001B[38;5;241m=\u001B[39m blend_edges(image_rgb, stable_diffused_result, mask)\n\u001B[1;32m    181\u001B[0m save_results(mask, stable_diffused_result, blended_result)\n",
      "Cell \u001B[0;32mIn[1], line 127\u001B[0m, in \u001B[0;36mstable_diffusion_inpainting\u001B[0;34m(image_rgb, mask)\u001B[0m\n\u001B[1;32m    124\u001B[0m mask_pil \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(mask)\u001B[38;5;241m.\u001B[39mresize((width, height))\n\u001B[1;32m    126\u001B[0m \u001B[38;5;66;03m# Run Stable Diffusion inpainting\u001B[39;00m\n\u001B[0;32m--> 127\u001B[0m inpainted_image \u001B[38;5;241m=\u001B[39m \u001B[43mpipe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    128\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    129\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mFill the missing background of the image naturally. \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m    130\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mMaintain proper background lighting.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m    131\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    132\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnegative_prompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    133\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mDo not introduce any new objects, patterns, or artificial elements.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m    134\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_pil\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmask_image\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask_pil\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    137\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwidth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwidth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    139\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_inference_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\n\u001B[1;32m    140\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mimages[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    142\u001B[0m \u001B[38;5;66;03m# Convert result to NumPy array\u001B[39;00m\n\u001B[1;32m    143\u001B[0m inpainted_array \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(inpainted_image)\n",
      "File \u001B[0;32m~/PycharmProjects/GAI/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/GAI/.venv/lib/python3.12/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py:1255\u001B[0m, in \u001B[0;36mStableDiffusionInpaintPipeline.__call__\u001B[0;34m(self, prompt, image, mask_image, masked_image_latents, height, width, padding_mask_crop, strength, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1252\u001B[0m     latent_model_input \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([latent_model_input, mask, masked_image_latents], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m   1254\u001B[0m \u001B[38;5;66;03m# predict the noise residual\u001B[39;00m\n\u001B[0;32m-> 1255\u001B[0m noise_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munet\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1256\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlatent_model_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1258\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprompt_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1259\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimestep_cond\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimestep_cond\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1260\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcross_attention_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_attention_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1261\u001B[0m \u001B[43m    \u001B[49m\u001B[43madded_cond_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madded_cond_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1262\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1263\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1265\u001B[0m \u001B[38;5;66;03m# perform guidance\u001B[39;00m\n\u001B[1;32m   1266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_classifier_free_guidance:\n",
      "File \u001B[0;32m~/PycharmProjects/GAI/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/GAI/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/GAI/.venv/lib/python3.12/site-packages/diffusers/models/unets/unet_2d_condition.py:1292\u001B[0m, in \u001B[0;36mUNet2DConditionModel.forward\u001B[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001B[0m\n\u001B[1;32m   1281\u001B[0m         sample \u001B[38;5;241m=\u001B[39m upsample_block(\n\u001B[1;32m   1282\u001B[0m             hidden_states\u001B[38;5;241m=\u001B[39msample,\n\u001B[1;32m   1283\u001B[0m             temb\u001B[38;5;241m=\u001B[39memb,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1289\u001B[0m             encoder_attention_mask\u001B[38;5;241m=\u001B[39mencoder_attention_mask,\n\u001B[1;32m   1290\u001B[0m         )\n\u001B[1;32m   1291\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1292\u001B[0m         sample \u001B[38;5;241m=\u001B[39m \u001B[43mupsample_block\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1293\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1294\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtemb\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43memb\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1295\u001B[0m \u001B[43m            \u001B[49m\u001B[43mres_hidden_states_tuple\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mres_samples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1296\u001B[0m \u001B[43m            \u001B[49m\u001B[43mupsample_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mupsample_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1297\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1299\u001B[0m \u001B[38;5;66;03m# 6. post-process\u001B[39;00m\n\u001B[1;32m   1300\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv_norm_out:\n",
      "File \u001B[0;32m~/PycharmProjects/GAI/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/GAI/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/GAI/.venv/lib/python3.12/site-packages/diffusers/models/unets/unet_2d_blocks.py:2740\u001B[0m, in \u001B[0;36mUpBlock2D.forward\u001B[0;34m(self, hidden_states, res_hidden_states_tuple, temb, upsample_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2736\u001B[0m             hidden_states \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m   2737\u001B[0m                 create_custom_forward(resnet), hidden_states, temb\n\u001B[1;32m   2738\u001B[0m             )\n\u001B[1;32m   2739\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2740\u001B[0m         hidden_states \u001B[38;5;241m=\u001B[39m \u001B[43mresnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupsamplers \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2743\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m upsampler \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupsamplers:\n",
      "File \u001B[0;32m~/PycharmProjects/GAI/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/GAI/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/GAI/.venv/lib/python3.12/site-packages/diffusers/models/resnet.py:327\u001B[0m, in \u001B[0;36mResnetBlock2D.forward\u001B[0;34m(self, input_tensor, temb, *args, **kwargs)\u001B[0m\n\u001B[1;32m    323\u001B[0m     deprecate(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscale\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1.0.0\u001B[39m\u001B[38;5;124m\"\u001B[39m, deprecation_message)\n\u001B[1;32m    325\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m input_tensor\n\u001B[0;32m--> 327\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    328\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnonlinearity(hidden_states)\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupsample \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    331\u001B[0m     \u001B[38;5;66;03m# upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/GAI/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/GAI/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/GAI/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:313\u001B[0m, in \u001B[0;36mGroupNorm.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 313\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroup_norm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_groups\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/GAI/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2965\u001B[0m, in \u001B[0;36mgroup_norm\u001B[0;34m(input, num_groups, weight, bias, eps)\u001B[0m\n\u001B[1;32m   2958\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   2959\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected at least 2 dimensions for input tensor but received \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2960\u001B[0m     )\n\u001B[1;32m   2961\u001B[0m _verify_batch_size(\n\u001B[1;32m   2962\u001B[0m     [\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m num_groups, num_groups]\n\u001B[1;32m   2963\u001B[0m     \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m2\u001B[39m:])\n\u001B[1;32m   2964\u001B[0m )\n\u001B[0;32m-> 2965\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroup_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2966\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_groups\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcudnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menabled\u001B[49m\n\u001B[1;32m   2967\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/GAI/.venv/lib/python3.12/site-packages/torch/_refs/__init__.py:3212\u001B[0m, in \u001B[0;36mnative_group_norm\u001B[0;34m(input, weight, bias, batch_size, num_channels, flattened_inner_size, num_groups, eps)\u001B[0m\n\u001B[1;32m   3207\u001B[0m input_reshaped \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mreshape(\n\u001B[1;32m   3208\u001B[0m     \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m   3209\u001B[0m     [batch_size, num_groups, num_channels \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m num_groups, flattened_inner_size],\n\u001B[1;32m   3210\u001B[0m )\n\u001B[1;32m   3211\u001B[0m out, mean, rstd \u001B[38;5;241m=\u001B[39m _normalize(input_reshaped, reduction_dims, eps)\n\u001B[0;32m-> 3212\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mout\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3214\u001B[0m broadcast_dims \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mndim))\n\u001B[1;32m   3215\u001B[0m unsqueeze_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
